{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5320f02f-deaf-44d3-93c6-2579150561f3",
   "metadata": {},
   "source": [
    "# Brain2Img \n",
    "\n",
    "First attempt to decode the brain activation maps to the latent space\n",
    "\n",
    "I will try with different approaches:\n",
    "\n",
    "1) Linear mapping between fmri and latent space of VAE, VAEGAN and PGGAN\n",
    "2) Convolutional network (optimizable) to map fmri to the latent spaces\n",
    "3) Transformer approach to VQVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2b1b06b-b6bc-4597-bc09-302249b46537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import Compose,ToTensor,Resize,PILToTensor\n",
    "from torchsummary import summary \n",
    "\n",
    "import monai\n",
    "from monai.transforms import (\n",
    "    AddChannel,\n",
    "    Compose,\n",
    "    LoadImage,\n",
    "    RandSpatialCrop,\n",
    "    ScaleIntensity,\n",
    "    EnsureType,\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from os.path import join as opj\n",
    "import glob\n",
    "\n",
    "from classes.VAE import VAE, VAEGAN\n",
    "from classes.VQVAE import VQVAE, VQVAE2\n",
    "\n",
    "\n",
    "#from utils.utils import SeparableConv3D\n",
    "from imutils import build_montages\n",
    "import tqdm\n",
    "from tqdm import notebook\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torchvision.io import read_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "455316a1-2387-4dd1-9191-fefae87bfefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01a948d3-99f3-4093-9b17-18ae7d4e81d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing  88\n"
     ]
    }
   ],
   "source": [
    "activation_path=\"/home/matteo/NeuroGEN/ds_proc/sub-01_activation\"\n",
    "activations=glob.glob(opj(activation_path,\"*.nii.gz\"))\n",
    "\n",
    "idx=activations.index('/home/matteo/NeuroGEN/ds_proc/sub-01_activation/activation_constant.nii.gz')\n",
    "\n",
    "print(\"removing \", idx)\n",
    "del activations[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cc78b85-3ace-42fa-87f9-78512f2c3ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: \t5921\n",
      "Val: \t1268\n",
      "Test: \t1270\n"
     ]
    }
   ],
   "source": [
    "train_fraction=0.70\n",
    "val_fraction=0.15\n",
    "test_fraction=0.15\n",
    "\n",
    "train_idx=int(len(activations)*train_fraction)\n",
    "val_idx=int(train_idx+len(activations)*val_fraction)\n",
    "test_idx=int(val_idx+len(activations)*test_fraction)\n",
    "\n",
    "train_activations=activations[:train_idx]\n",
    "val_activations=activations[train_idx:val_idx]\n",
    "test_activations=activations[val_idx:]\n",
    "\n",
    "print(f\"Train: \\t{len(train_activations)}\\nVal: \\t{len(val_activations)}\\nTest: \\t{len(test_activations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f6e7623-9328-4132-869a-be5ca8b9678f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_path=\"models/vae\"\n",
    "vaegan_path=\"/models/vaegan\"\n",
    "vqvae_path=\"/models/vqvae\"\n",
    "\n",
    "BS=4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d5d3ce-a130-40ca-91fc-9a72e3de4de2",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "696e88a0-950c-40f8-9149-60c6042aab89",
   "metadata": {},
   "outputs": [],
   "source": [
    "check=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "054cf712-1a32-4128-a5d2-a018ed9f6d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_img_path(name,basepath=\"/home/matteo/NeuroGEN/Dataset/Img/img_align_fmri_stimuli\"):\n",
    "    s=find_between(name,\"activation_\",\".nii.gz\")\n",
    "    imagePath=opj(basepath,s)\n",
    "    return imagePath\n",
    "\n",
    "def find_between( s, first, last ):\n",
    "    try:\n",
    "        start = s.index( first ) + len( first )\n",
    "        end = s.index( last, start )\n",
    "        return s[start:end]\n",
    "    except ValueError:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "class BrainDataset(Dataset):\n",
    "    \"\"\"Brain2img dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, activations, nifti_transform=None,image_transform=None):\n",
    "\n",
    "        self.activations = activations\n",
    "        self.images=[get_img_path(i) for i in activations]\n",
    "        \n",
    "        self.nifti_transform = nifti_transform\n",
    "        self.image_transform = image_transform\n",
    "        \n",
    "        self.niftireader=LoadImage(image_only=True)\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.activations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        \n",
    "        \n",
    "        act=self.niftireader(self.activations[idx])\n",
    "        img=read_image(self.images[idx])\n",
    "        \n",
    "        if self.image_transform is not None:\n",
    "            img = self.image_transform(img)/255.        \n",
    "        if self.nifti_transform is not None:\n",
    "            act = self.nifti_transform(act)\n",
    "        \n",
    "\n",
    "        act=torch.unsqueeze(act,0)\n",
    "        return act,img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "425a8f4b-fd72-448f-90f8-90685c3899d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transforms=Compose([\n",
    " Resize((128,128)),\n",
    "])\n",
    "nifti_transforms=Compose([ToTensor()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa149cec-9a26-40a3-9af0-d16eecc09223",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BrainDataset(train_activations,image_transform=image_transforms,nifti_transform=nifti_transforms)\n",
    "val_dataset = BrainDataset(train_activations,image_transform=image_transforms,nifti_transform=nifti_transforms)\n",
    "test_dataset = BrainDataset(train_activations,image_transform=image_transforms,nifti_transform=nifti_transforms)\n",
    "\n",
    "train_dataloader=DataLoader(train_dataset,batch_size=BS,shuffle=True)\n",
    "val_dataloader=DataLoader(val_dataset,batch_size=BS,shuffle=True)\n",
    "test_dataloader=DataLoader(test_dataset,batch_size=BS,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb3a8c98-1a19-4af5-a792-86d5e174de58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check\n",
    "if check:\n",
    "    a,i=next(iter(train_dataloader))\n",
    "    a.shape,i.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabf9abb-b948-453d-8c0c-8a4a99ca17e2",
   "metadata": {},
   "source": [
    "# VAE Approach\n",
    "\n",
    "3D convolutional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b25fbd1-be9d-4d01-87a9-a0dd93f3e65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matteo/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## VAE\n",
    "\n",
    "encoder_architecture=[[1,512],[1,256],[1,128],[1,64],[1,32]]\n",
    "decoder_architecture=[[3,64],[2,128],[1,256],[0,384],[0,512]]\n",
    "\n",
    "latent_dim=1024\n",
    "input_dim=(3,128,128)\n",
    "\n",
    "vae=VAE(input_dim=input_dim,latent_dim=latent_dim,encoder_architecture=encoder_architecture,decoder_architecture=decoder_architecture)\n",
    "\n",
    "vae.load_state_dict(torch.load(r\"models/vae/model.ckp\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3492a160-2c36-4697-bdf5-32fee7cf7f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class convBrain(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model=nn.Sequential(\n",
    "            nn.Conv3d(in_channels=1,out_channels=32,kernel_size=4,padding=1,stride=2),\n",
    "            nn.Tanh(),\n",
    "            nn.Conv3d(in_channels=32,out_channels=64,kernel_size=4,padding=1,stride=2),\n",
    "            nn.Tanh(),\n",
    "            nn.Conv3d(in_channels=64,out_channels=128,kernel_size=4,padding=1,stride=2),\n",
    "            nn.Tanh(),\n",
    "            nn.Conv3d(in_channels=128,out_channels=2,kernel_size=5,padding=\"same\"),\n",
    "            nn.Flatten(),\n",
    "            nn.LazyLinear(1024),\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37089149-cde9-4cf9-abb4-47f5492be35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "convbrain=convBrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0d7af83-695a-4db8-ba76-41b31a68f486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv3d-1       [-1, 32, 20, 40, 40]           2,080\n",
      "              Tanh-2       [-1, 32, 20, 40, 40]               0\n",
      "            Conv3d-3       [-1, 64, 10, 20, 20]         131,136\n",
      "              Tanh-4       [-1, 64, 10, 20, 20]               0\n",
      "            Conv3d-5       [-1, 128, 5, 10, 10]         524,416\n",
      "              Tanh-6       [-1, 128, 5, 10, 10]               0\n",
      "            Conv3d-7         [-1, 2, 5, 10, 10]          32,002\n",
      "           Flatten-8                 [-1, 1000]               0\n",
      "            Linear-9                 [-1, 1024]       1,025,024\n",
      "================================================================\n",
      "Total params: 1,714,658\n",
      "Trainable params: 1,714,658\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.00\n",
      "Forward/backward pass size (MB): 20.53\n",
      "Params size (MB): 6.54\n",
      "Estimated Total Size (MB): 28.07\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(convbrain,(1,41,80,80),device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf48e8a5-9a0b-4080-a3c7-da3e206434e4",
   "metadata": {},
   "source": [
    "## End-to-end model\n",
    "\n",
    "With pretrained VAE's decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7225003-99f4-4c70-9bd8-4344d16c9f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class brain2vae(nn.Module):\n",
    "    def __init__(self,brain_model,decoder_model):\n",
    "        super().__init__()\n",
    "        self.brain_model=brain_model\n",
    "        self.decoder_model=decoder_model\n",
    "        \n",
    "    def forward(self,x):\n",
    "        z=self.brain_model(x)\n",
    "        x=self.decoder_model(z)\n",
    "        return x\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6173763-ea3f-4a15-9d89-7a4d8da408c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## check ricostruzioni\n",
    "\n",
    "if check:\n",
    "    with torch.no_grad():\n",
    "        ir=vae(i.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "082a52bd-10fa-4f42-8729-5309c10aa578",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check output modello\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "model=brain2vae(convbrain,vae.decoder).to(device)\n",
    "\n",
    "if check:\n",
    "    with torch.no_grad():\n",
    "        out=model(a.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42ca416-3276-4888-88f9-ed4b5737eb70",
   "metadata": {},
   "source": [
    "### Reconstruction check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c29e7dbe-e614-426e-9fff-7eafb043eeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if check:\n",
    "    fig,axs=plt.subplots(2,4,figsize=(10,4))\n",
    "\n",
    "    for idx in range(4):\n",
    "        axs[0,idx].imshow(i[idx].detach().cpu().permute(1,2,0))\n",
    "        axs[1,idx].imshow(ir[idx].detach().cpu().permute(1,2,0))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadc950e-d4db-4b86-bad7-5578a1f8440e",
   "metadata": {},
   "source": [
    "### Random reconstruction with model initiliazed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6db57e3-4754-4881-aeaa-d96b2db47758",
   "metadata": {},
   "outputs": [],
   "source": [
    "if check:\n",
    "    fig,axs=plt.subplots(1,2,figsize=(10,10))\n",
    "    axs[0].imshow(i[0].permute(1,2,0).detach().cpu())\n",
    "    axs[1].imshow(out[0].permute(1,2,0).detach().cpu())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f843a8-a806-4a82-843b-1fd66fbe3624",
   "metadata": {},
   "source": [
    "## Training loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b4b4c6e1-d9b2-46b2-aab2-598cc9092c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "optimizer=torch.optim.Adam(convbrain.parameters(),lr=1e-3)\n",
    "criterion=nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "33750a17-2887-4df9-b37f-1ca40d2052a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,train_dataloader,epochs=10,optimizer=optimizer,val_dataloader=None,device=device):\n",
    "    loss_history=[]\n",
    "    for epoch in range(epochs):\n",
    "        loss_temp=[]\n",
    "        model.train()\n",
    "        with notebook.tqdm(train_dataloader, unit=\"batch\") as tepoch:\n",
    "\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "            for x,y in tepoch:\n",
    "\n",
    "                x,y=x.to(device),y.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                y_pred=model(x)\n",
    "\n",
    "                loss=criterion(y_pred,y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                tepoch.set_postfix(loss=loss.item())\n",
    "                loss_temp.append(loss.item())\n",
    "        loss_history.append(np.mean(loss_temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831975fb-a090-4e4d-8bbe-33533aa32396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa1978b0b7c746e1b538ec90f3f26a96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1481 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cacc5a7896ea45448c079e2eb90d5661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1481 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "674e0fdb33d04195a0ab97db74933a51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1481 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "918202359b86433a92cf13a48619f8ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1481 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaf79afcf98d4a6e8e64e22617e7bcbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1481 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1802a8e438654c21813a8ddf2cd25494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1481 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(model,train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d50ff11-a07a-4d0e-bcc8-8f2b7baa7e87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
